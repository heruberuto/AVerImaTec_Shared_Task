{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9322c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import json, random,os\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "\n",
    "DATA_DIR = \"/mnt/data/factcheck/averimatec\"\n",
    "SPLIT = \"val\"\n",
    "\n",
    "model = SentenceTransformer(\"clip-ViT-B-32\", device=\"cuda\")\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={\"device\": \"cuda:0\"})\n",
    "random.seed(111)\n",
    "\n",
    "with open(f\"{DATA_DIR}/{SPLIT}.json\") as f:\n",
    "    datapoints = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75bcdaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'https://web.archive.org/web/20230629073625/https://leadstories.com/hoax-alert/2023/06/fact-check-photos-do-not-prove-hobby-lobby-is-selling-satanic-statues.html',\n",
       " 'date': '2023-06-13',\n",
       " 'label': 'Refuted',\n",
       " 'location': 'US',\n",
       " 'questions': [{'input_images': [],\n",
       "   'answers': [{'answer_type': 'Extractive',\n",
       "     'answer_text': 'Hobby Lobby Stores, Inc., formerly Hobby Lobby Creative Centers, is an American retail company. It owns a chain of arts and crafts stores with a volume of over $5 billion in 2018.',\n",
       "     'source_url': 'https://en.wikipedia.org/wiki/Hobby_Lobby',\n",
       "     'source_medium': 'Web text'}],\n",
       "   'question': 'What is Hobby Lobby?',\n",
       "   'answer_method': 'Text-search',\n",
       "   'question_type': ['Text-related']},\n",
       "  {'input_images': ['6787dee1e2f02e5f4981b14e#INPUT#1#0.jpg',\n",
       "    '6787dee1e2f02e5f4981b14e#INPUT#1#1.jpg',\n",
       "    '6787dee1e2f02e5f4981b14e#INPUT#1#2.jpg',\n",
       "    '6787dee1e2f02e5f4981b14e#INPUT#1#3.jpg'],\n",
       "   'answers': [{'answer_type': 'Abstractive',\n",
       "     'answer_text': 'They are AI images created by Waster_Space. This is their comment on the images: Hobby Lobby if they had \"Satanic\" merchandise...(literally done in 5 mins...if I had known the images would be shared so many times, I would have spent more time on them)',\n",
       "     'source_url': 'https://www.artzone.ai/images/7f6126f1-967e-4c79-860f-f19a6eef290f',\n",
       "     'source_medium': 'Web text'}],\n",
       "   'question': 'What stores are these images from?',\n",
       "   'answer_method': 'Text-search',\n",
       "   'question_type': ['Image-related']}],\n",
       " 'justification': 'The claim is refuted by proving that the images were created with AI by Waster_Space for satirical purposes.',\n",
       " 'claim_text': 'Photos show that the craft store chain Hobby Lobby is selling demonic statues. ',\n",
       " 'claim_images': ['6787dee1e2f02e5f4981b14e#CLAIM#0.jpg',\n",
       "  '6787dee1e2f02e5f4981b14e#CLAIM#1.jpg',\n",
       "  '6787dee1e2f02e5f4981b14e#CLAIM#2.jpg',\n",
       "  '6787dee1e2f02e5f4981b14e#CLAIM#3.jpg'],\n",
       " 'metadata': {'speaker': 'Carousel Studios',\n",
       "  'transcription': '',\n",
       "  'media_source': '',\n",
       "  'original_claim_url': '',\n",
       "  'reporting_source': 'Facebook ',\n",
       "  'claim_types': ['Event/Property Claim'],\n",
       "  'fact_checking_strategies': ['Written Evidence',\n",
       "   'Reverse Image Search',\n",
       "   'Fact-checker Reference'],\n",
       "  'modality': 'Image-text',\n",
       "  'refuting_reasons': ['Misuse of images'],\n",
       "  'image_misuse_types': ['Out-of-context'],\n",
       "  'image_used': 'Yes'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78a4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def generate_clip_embeddings(images_path, model):\n",
    "\n",
    "    image_paths = glob(os.path.join(images_path, \"**/*.jpg\"), recursive=True)\n",
    "\n",
    "    embeddings = []\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            embedding = model.encode(image)\n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image {img_path}: {e}\")\n",
    "\n",
    "    return embeddings, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996057b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_faiss_index(embeddings, image_paths, output_path):\n",
    "\n",
    "    dimension = len(embeddings[0])\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index = faiss.IndexIDMap(index)\n",
    "\n",
    "    vectors = np.array(embeddings).astype(np.float32)\n",
    "\n",
    "    # Add vectors to the index with IDs\n",
    "    index.add_with_ids(vectors, np.array(range(len(embeddings))))\n",
    "\n",
    "    # Save the index\n",
    "    faiss.write_index(index, output_path)\n",
    "    print(f\"Index created and saved to {output_path}\")\n",
    "\n",
    "    # Save image paths\n",
    "    with open(output_path + \".paths\", \"w\") as f:\n",
    "        for img_path in image_paths:\n",
    "            f.write(img_path + \"\\n\")\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3b6ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c759fc09bd7d4f5d9cea56e3d82c9c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The channel dimension is ambiguous. Got image shape (1, 1, 3). Assuming channels are the first dimension. Use the [input_data_format](https://huggingface.co/docs/transformers/main/internal/image_processing_utils#transformers.image_transforms.rescale.input_data_format) parameter to assign the channel dimension.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encoding image /mnt/data/factcheck/averimatec/knowledge_store/val/image_related/image_related_store_image_val/17/127.jpg: mean must have 1 elements if it is an iterable, got 3\n",
      "Index created and saved to /mnt/data/factcheck/averimatec/vector_store/val/image/17/faiss.index\n",
      "Index created and saved to /mnt/data/factcheck/averimatec/vector_store/val/image/18/faiss.index\n"
     ]
    }
   ],
   "source": [
    "for CLAIM_ID in tqdm(range(15,19)):# Naive version with \\n concatenated url2texts:   \n",
    "    # skip if f\"{DATA_DIR}/data_store/vecstore/{SPLIT}/4k/{CLAIM_ID}\" exists\n",
    "    if os.path.exists(f\"{DATA_DIR}/vector_store/{SPLIT}/image/{CLAIM_ID}\"):\n",
    "        continue\n",
    "\n",
    "    os.makedirs(f\"{DATA_DIR}/vector_store/{SPLIT}/image/{CLAIM_ID}\")\n",
    "    datapoint = datapoints[CLAIM_ID]\n",
    "    claim = datapoint[\"claim_text\"]\n",
    "    # display(Markdown(\"### ðŸ—¯ï¸ \" + claim + \" [\" + datapoint[\"label\"] + \"]\"))\n",
    "    embeddings, image_paths = generate_clip_embeddings(\n",
    "        f\"{DATA_DIR}/knowledge_store/{SPLIT}/image_related/image_related_store_image_{SPLIT}/{CLAIM_ID}\",model\n",
    "    )\n",
    "    index = create_faiss_index(embeddings, image_paths, f\"{DATA_DIR}/vector_store/{SPLIT}/image/{CLAIM_ID}/faiss.index\")\n",
    "    #db = FAISS.from_documents(chunks_pruned, embeddings)\n",
    "#    db.save_local(f\"{DATA_DIR}/vector_store/{SPLIT}/image/{CLAIM_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_images(query, model, index, image_paths, top_k=3):\n",
    "\n",
    "    # query preprocess:\n",
    "    if query.endswith(\n",
    "        (\".png\", \".jpg\", \".jpeg\", \".tiff\", \".bmp\", \".gif\", \".webp\")\n",
    "    ):  # Check if the query is an image file\n",
    "        query = Image.open(query)\n",
    "\n",
    "    query_features = model.encode(query)\n",
    "    query_features = query_features.astype(np.float32).reshape(1, -1)\n",
    "\n",
    "    distances, indices = index.search(query_features, top_k)\n",
    "\n",
    "    retrieved_images = [image_paths[int(idx)] for idx in indices[0]]\n",
    "\n",
    "    return query, retrieved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_results(query, retrieved_images):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # If image query\n",
    "    if isinstance(query, Image.Image):\n",
    "        plt.subplot(1, len(retrieved_images) + 1, 1)\n",
    "        plt.imshow(query)\n",
    "        plt.title(\"Query Image\")\n",
    "        plt.axis(\"off\")\n",
    "        start_idx = 2\n",
    "\n",
    "    # If text query\n",
    "    else:\n",
    "        plt.subplot(1, len(retrieved_images) + 1, 1)\n",
    "        plt.text(0.5, 0.5, f\"Query:\\n\\n '{query}'\", fontsize=16, ha=\"center\", va=\"center\")\n",
    "        plt.axis(\"off\")\n",
    "        start_idx = 2\n",
    "\n",
    "    # Display images\n",
    "    for i, img_path in enumerate(retrieved_images):\n",
    "\n",
    "        plt.subplot(1, len(retrieved_images) + 1, i + start_idx)\n",
    "        plt.imshow(Image.open(img_path))\n",
    "        plt.title(f\"Match {i + 1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
