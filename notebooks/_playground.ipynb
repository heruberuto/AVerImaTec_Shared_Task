{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé Retrieval (and QG?) playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_core.example_selectors import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "fro m langchain_community.retrievers import KNNRetriever\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Error: pull model manifest: 412: \n",
      "\n",
      "The model you are attempting to pull requires a newer version of Ollama.\n",
      "\n",
      "Please download the latest version at:\n",
      "\n",
      "\thttps://ollama.com/download\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!ollama pull qwen3-vl:32b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 0. Choose claim & docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üóØÔ∏è The White House violated flag code by putting a LGBTQ+ pride flag in between two U.S. flags. [Refuted]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**https://www.fordlibrarymuseum.gov/the-fords/gerald-r-ford/key-speeches-and-writings-gerald-r-ford**\n",
       "\n",
       " * Pre-Presidential Speeches\n",
       "December 6, 1973\n",
       "Mr. Speaker, Mr. Chief Justice, Mr. President pro tempore, distinguished guests and friends:\n",
       "Together we have made history here today. For the first time we have carried out the command of the 25th Amendment.\n",
       "In exactly 8 weeks, we have demonstrated to the world that our great Republic stands solid, stands strong upon the bedrock of the Constitution.\n",
       "I am a Ford, not a Lincoln. My addresses will never be as eloquent as Mr. Lincoln's. But I will do my very best to equal his brevity and his plain speaking.\n",
       "I am deeply grateful to you, Mr. President, for the trust and the confidence your nomination implies.\n",
       "As I have throughout my public service under six administrations I will try to set a fine example of respect for the crushing and lonely burdens which the Nation lays upon the President of the United States.\n",
       "Mr. President, you have my support and my loyalty.\n",
       "To the Congress assembled, my former colleagues who have elected me on behalf of our fellow countrymen I express my heartfelt thanks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**https://mainelegion.org/pages/resources/links/the-flag.php**\n",
       "\n",
       " * The American Legion Flag Advocacy Flag Code Flag Questions & Answers Folding the Flag Frequency Asked Questions The Pledge of Allegiance Unserviceable Flags"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**https://give.hrc.org/page/23977/donate/1?locale=en-US**\n",
       "\n",
       " * Each and every day, the Human Rights Campaign flies our flag with pride above our Washington, D.C. offices.\n",
       "Our logo is one of the most recognizable symbols of the lesbian, gay, bisexual, transgender and queer community (LGBTQ) -- and has become synonymous with the fight for equality.\n",
       "Now you can honor a loved one or friend by flying the HRC flag above the heart of the nation's capital, only blocks away from the White House.\n",
       "Once your flag has been flown, we will send it to you with a certificate to commemorate your contribution to the fight for equality.\n",
       "Please allow 15 business days from the day your flag is flown for your shipment to arrive. Flag measures 3 ft. x 4 ft.\n",
       "Your flag will be shipped via UPS Ground. Domestic shipping only. No PO Boxes.\n",
       "If you have any questions, please contact [email protected].\n",
       "Please note that the minimum contribution for the HRC Flag Program is $150."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIR = \"/mnt/data/factcheck/averimatec\"\n",
    "SPLIT = \"val\"\n",
    "\n",
    "with open(f\"{DATA_DIR}/{SPLIT}.json\") as f:\n",
    "    datapoints = json.load(f)\n",
    "\n",
    "CLAIM_ID = random.randint(0,len(datapoints))\n",
    "datapoint = datapoints[CLAIM_ID]\n",
    "claim = datapoint[\"claim_text\"]\n",
    "docstore = []\n",
    "for line in open(\n",
    "    f\"{DATA_DIR}/knowledge_store/{SPLIT}/text_related/text_related_store_text_{SPLIT}/{CLAIM_ID}.json\"\n",
    "):\n",
    "    docstore.append(json.loads(line))\n",
    "\n",
    "# print claim in markdown with some sample evidence\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim + \" [\" + datapoint[\"label\"] + \"]\"))\n",
    "# sample 3\n",
    "for i in random.sample(range(len(docstore)), 3):\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{docstore[i]['url']}**\\n\\n * {newline.join(docstore[i]['url2text'][:10])}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è 0.1 Docstore formatting/scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive version with \\n concatenated url2texts:\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\" \".join(doc[\"url2text\"]),\n",
    "        metadata={\n",
    "            \"url\": doc[\"url\"],\n",
    "            # \"sentences\": doc[\"url2text\"]\n",
    "        },\n",
    "    )\n",
    "    for doc in docstore\n",
    "]\n",
    "\n",
    "TOKENS_PER_CHAR = 0.25\n",
    "EMBEDDING_INPUT_SIZE = 512\n",
    "\n",
    "chunks = []\n",
    "for doc in docstore:\n",
    "    buffer = \"\"\n",
    "    for i, sentence in enumerate(doc[\"url2text\"]):\n",
    "        if (\n",
    "            i == len(doc[\"url2text\"]) - 1\n",
    "            or len(buffer) + len(sentence) >= EMBEDDING_INPUT_SIZE / TOKENS_PER_CHAR\n",
    "        ):\n",
    "            context_before = \"\"\n",
    "            if chunks and chunks[-1].metadata[\"url\"] == doc[\"url\"]:\n",
    "                chunks[-1].metadata[\"context_after\"] = buffer\n",
    "                context_before = chunks[-1].page_content\n",
    "            chunks.append(\n",
    "                Document(\n",
    "                    page_content=buffer,\n",
    "                    metadata={\"url\": doc[\"url\"], \"context_before\": context_before, \"context_after\": \"\"},\n",
    "                )\n",
    "            )\n",
    "\n",
    "            buffer = \"\"\n",
    "        buffer += sentence + \" \"\n",
    "# chunk the documents into smaller pieces\n",
    "chunks[random.randint(0, len(chunks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chat import  pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chid = -1\n",
    "\n",
    "display(Markdown(chunks[chid].metadata[\"context_before\"]))\n",
    "display(Markdown(chunks[chid].page_content))\n",
    "display(Markdown(chunks[chid].metadata[\"context_after\"]))\n",
    "chunks[chid].metadata[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    chunks, k=2500\n",
    ")\n",
    "chunks_pruned = retriever.invoke(claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Docstore hist & truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks),len(chunks_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of sentences len, between 0 and 100\n",
    "plt.hist([len(doc.page_content) for doc in chunks], bins=50, range=(0, 3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of sentences len, between 0 and 100\n",
    "plt.hist([len(doc.page_content) for doc in chunks], bins=50, range=(0, 3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show doc with most sentences\n",
    "max_sentences = max(documents, key=lambda d: len(d.metadata[\"sentences\"]))\n",
    "max_sentences.metadata[\"url\"]\n",
    "max_sentences.page_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim document page contents to 13000 characters\n",
    "for doc in documents:\n",
    "    doc.page_content = doc.page_content[:13000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê 1. Embedding searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"Linq-AI-Research/Linq-Embed-Mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = KNNRetriever.from_documents(documents, embeddings, k=10)\n",
    "results = reranker.get_relevant_documents(claim)\n",
    "\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim))\n",
    "display(Markdown(\"*Retrieved by \" + embeddings.model_name + \"*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = KNNRetriever.from_documents(documents, embeddings, k=10)\n",
    "results = reranker.get_relevant_documents(claim)\n",
    "\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim))\n",
    "display(Markdown(\"*Retrieved by \" + embeddings.model_name + \"*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purge cuda mem completely\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìê Max. Marginal relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    chunks, k=3000\n",
    ")\n",
    "chunks_pruned = retriever.invoke(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = Chroma(f\"dev_{CLAIM_ID}_mxbai\", persist_directory=f\"data_store/vector_store_dev/{CLAIM_ID}\")\n",
    "chroma.delete_collection()\n",
    "documents_ = [doc.copy() for doc in chunks_pruned]\n",
    "for doc in documents_:\n",
    "    if 'sentences' in doc.metadata:\n",
    "        doc.metadata.pop(\"sentences\", None)\n",
    "    \n",
    "chroma = chroma.from_documents(documents_, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make or overwrite /home/ullriher/aic_averitec/data_store/vector_store_dev/CLAIM_ID and persist chroma there\n",
    "!mkdir -p /home/ullriher/aic_averitec/data_store/vector_store_dev/{CLAIM_ID}\n",
    "chroma(f\"/home/ullriher/aic_averitec/data_store/vector_store_dev/{CLAIM_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    chunks, k=6000\n",
    ")\n",
    "chunks_pruned = retriever.invoke(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(chunks_pruned, embeddings)\n",
    "db.save_local(f\"data_store/vecstore/dev/6k/{CLAIM_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.load_local(f\"data_store/vecstore/dev/6k/{CLAIM_ID}\", embeddings,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.chat import SimpleJSONChat\n",
    "\n",
    "chat = SimpleJSONChat(\n",
    "    model=\"gpt-4o\",\n",
    "    system_prompt=f\"\"\"Pretend you are a researcher who receives a claim where your goal is to be as unbiased as possible. There are two teams: Query Generation (your team) and Information Acquisition. The goal of your team is to generate pertinent queries based on the claim such that when queried on Google/Bing, the results will be accurate and helpful in finding relevant sources. The second team then uses those sources to further refine whether the claim is supported (true), unsupported (false), or cherry picked (conflicting evidence). In summary, you will receive a claim and the task is to generate 10 queries that are relevant to the claim, paying mind to the different possible labels (i.e ensure the generated queries cover all possible labels). A necessary requirement is to include metadata like claim date, speaker, and reporting source. Use your imagination and take time to be thoughtful with queries, ensuring relevance to the claim. The queries should be formatted in a manner ready for querying via Google/Bing API, so no need for extra text or explanations intended for a user.\"\"\",\n",
    "    parse_output=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(claim+f\" (Speaker: {datapoint['speaker']}, {datapoint['claim_date']})\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(res):\n",
    "    result = []\n",
    "    lines = res.strip().split(\"\\n\")\n",
    "\n",
    "    # Parse each line to extract the content\n",
    "    parsed_results = [line.split(\". \")[1].strip('\"') for line in lines]\n",
    "\n",
    "    # Print the parsed results\n",
    "    for item in parsed_results:\n",
    "        result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAIM_ID = 253\n",
    "CLAIM_ID = random.choice(range(500))\n",
    "datapoint = json.load(open(\"data/dev.json\"))[CLAIM_ID]\n",
    "claim = datapoint[\"claim\"]\n",
    "\n",
    "res = chat(claim+f\" (Speaker: {datapoint['speaker']}, {datapoint['claim_date']})\")\n",
    "db = FAISS.load_local(f\"data_store/vecstore/dev/6k/{CLAIM_ID}\", embeddings,allow_dangerous_deserialization=True)\n",
    "\n",
    "display(Markdown(f\"## üóØÔ∏è {CLAIM_ID}: {claim} [{datapoint['label']}]\"))\n",
    "\n",
    "for query in parse(res):\n",
    "    results = db.similarity_search(query, 2)\n",
    "\n",
    "    display(Markdown(\"### üîé \" + query))\n",
    "    # sample 3\n",
    "    for r in results:\n",
    "        newline = \"\\n\"\n",
    "        display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search(claim, 10)\n",
    "\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim))\n",
    "display(Markdown(\"*Retrieved by \" + embeddings.model_name + \" (mmr Œª=0.5)*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.max_marginal_relevance_search(claim, 10, 40, .5)\n",
    "\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim))\n",
    "display(Markdown(\"*Retrieved by \" + embeddings.model_name + \" (mmr Œª=0.5)*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chroma.max_marginal_relevance_search(claim, 10, 40, .5)\n",
    "\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim))\n",
    "display(Markdown(\"*Retrieved by \" + embeddings.model_name + \" (mmr Œª=0.5)*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùå Cross-encoder re-ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, here we use our base sized model\n",
    "model_name = \"mixedbread-ai/mxbai-rerank-large-v1\"\n",
    "model = CrossEncoder(model_name)\n",
    "\n",
    "# Example query and document\n",
    "\n",
    "# Lets get the scores\n",
    "results = model.rank(claim, [doc.page_content for doc in chunks_pruned], return_documents=True, top_k=10)\n",
    "\n",
    "\n",
    "display(Markdown(\"### üóØÔ∏è \" + claim))\n",
    "display(Markdown(\"*Retrieved by \" + model_name + \"*\\n\\n\"))\n",
    "# sample 3\n",
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    r = chunks_pruned[r['corpus_id']]\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content[:256]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in results:\n",
    "    newline = \"\\n\"\n",
    "    r = chunks_pruned[r['corpus_id']]\n",
    "    display(Markdown(f\"**{r.metadata['url']}**\\n\\n{r.page_content}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
