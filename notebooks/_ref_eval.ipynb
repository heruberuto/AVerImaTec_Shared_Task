{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1ae2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import re\n",
    "import random\n",
    "\n",
    "root_dir = os.path.abspath(\"\")\n",
    "text_val_demo = open(os.path.join(root_dir, \"templates/evid_evaluation_text.txt\")).readlines()\n",
    "text_val_demo = \"\".join(text_val_demo)\n",
    "\n",
    "seperate_val_demo = open(os.path.join(root_dir, \"templates/evid_evaluation_text_seperate.txt\")).readlines()\n",
    "seperate_val_demo = \"\".join(seperate_val_demo)\n",
    "\n",
    "from templates import evid_evaluation_joint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "joint_val_demo = \"\".join(\n",
    "    [\n",
    "        evid_evaluation_joint.instruction,\n",
    "        evid_evaluation_joint.exp_first,\n",
    "        evid_evaluation_joint.exp_second,\n",
    "    ]\n",
    ")\n",
    "\n",
    "ques_val_demo = \"\".join(open(os.path.join(root_dir, \"templates/ques_evaluation_text.txt\")).readlines())\n",
    "justi_val_demo = \"\".join(open(os.path.join(root_dir, \"templates/justi_evaluation_text.txt\")).readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde85371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You will get as input a reference evidence ([REF]) and a predicted evidence ([PRED]). [IMG_1], [IMG_2] ... are placeholders for images. Two facts may be textually aligned. But if they have different images, the two facts could be irrelevant.\\nPlease verify the correctness of the predicted evidence by comparing it to the reference evidence, following these steps:\\n1. Evaluate each fact in the predicted evidence individually: is the fact supported by the REFERENCE evidence? Do not use additional sources or background knowledge.\\n2. Evaluate each fact in the reference evidence individually: is the fact supported by the PREDICTED evidence? Do not use additional sources or background knowledge.\\n3. Finally summarise (1.) how many predicted facts are supported by the reference evidence and explanations([PRED in REF] and [PRED in REF Exp]), (2.) how many reference facts are supported by the predicted evidence and explanations ([REF in PRED] and [REF in PRED Exp]).\\nGenerate the output as shown in the examples below:\\n[PRED]: 1. The missle in [IMG_1] is Fateh 110. 2. Ilan Omar has attended the training in [IMG_2]. 3. Prince Phillip wore the Royal Guard uniform in Jan. 14, 2003. 4.The raid in Washington took place on Saturday, Oct. 26, 1999.\\n[REF]: 1. [IMG_1] was taken in Jan. 20, 2003. 2. No evidence can be found related to the type of missle in [IMG_2]. 3. The woman in [IMG_3] for a training is not Ilan Omar. 4. No answer was found regarding when the raid in Washington took place. 5. Prince Phillip wore the Royal Guard uniform shown in [IMG_4] previously in Jan. 2003.\\n[PRED in REF]: 1\\n[PRED in REF Exp]: 1. It is refuted by the second fact in the predicted evidence set. 2. No related facts can be found in the reference set. Though the text part of the thrid fact in the reference set is similar to it, they have different images. 3. The fact is supported by the fifth fact in the evidence set. 4. It is refuted by the fourth fact in the reference evidence set.\\n[REF in PRED]: 0\\n[REF in PRED Exp]: 1. No related facts can be found in the reference set. Though the textual part of the fact aligns with the third fact in the predicted evidence, they have different images. 2. It is refuted by the first fact in the predicted evidence set. 3. No related facts can be found in the reference set. Though the text part of the second fact in the predicted evidence set is similar to it, they have different images. 4. It is refuted by the fourth fact in the predicted evidence which claims the date of the raid could be found. 5.  No related facts can be found in the reference set.  Though the text part of the third fact in the predicted evidence set is similar to it, they have different images.\\n[PRED]: 1. [IMG_1] was taken on Jan. 19, 2025. 2. The current view of the benches in [IMG_2] is [IMG_3]. 3. The date of the claim is Nov. 22, 2023.\\n[REF]: 1. The claim was made on Jan. 22, 2021. 2. [IMG_2] was taken on Jan. 19, 2025. 3. The benches in [IMG_1] currently look like [IMG_3]. 4.Trump dressed as [IMG_1] in the meeting.\\n[PRED in REF]: 2\\n[PRED in REF Exp]: 1. The second piece of evidence in the reference evidence supports it. 2. The third evidence in the evidence set has a similar meaning to this fact. 3. The fact claims the date as Nov. 22, 2023, which is different from the first fact in the refence evidence, Jan. 22, 2021.\\n[REF in PRED]: 2\\n[REF in PRED Exp]: 1. The fact claims the date as Jan. 22, 2021, which is different from the third fact in the predicted evidence, Nov. 22, 2023. 2. It is supported by the first fact in the predicted evidence. 3. It is supported by the third fact in the predicted evidence. 4. No related facts can be found in the predicted evidence set.\\n\\nReturn the output in the exact format as specified in the examples, do not generate any additional output:\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_val_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ab559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_by_words(text, word_list):\n",
    "    # Create a regex pattern with word boundaries for each word in the list\n",
    "    pattern = r\"(\" + r\"|\".join(map(re.escape, word_list)) + r\")\"\n",
    "    # Use re.split to split the text and keep the delimiters\n",
    "    split_result = re.split(pattern, text)\n",
    "    # Remove empty strings and strip spaces\n",
    "    split_result = [s.strip() for s in split_result if s.strip()]\n",
    "    return split_result\n",
    "\n",
    "\n",
    "def gen_incontext_input_textonly(pred, ref, demos):\n",
    "    texts = []\n",
    "    texts.append(demos)\n",
    "    texts.append(\"\\n[PRED]: \" + pred)\n",
    "    texts.append(\"[REF]: \" + ref)\n",
    "    texts = \"\\n\".join(texts)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def score_extraction(feedback):\n",
    "    pred_in_ref = feedback.split(\"[PRED in REF]: \")[-1].split(\"\\n\")[0].split(\";\")[0].strip()\n",
    "    ref_in_pred = feedback.split(\"[REF in PRED]: \")[-1].split(\"\\n\")[0].split(\";\")[0].strip()\n",
    "    if pred_in_ref.isdigit():\n",
    "        pred_in_ref = int(pred_in_ref)\n",
    "    else:\n",
    "        pred_in_ref = 0\n",
    "    if ref_in_pred.isdigit():\n",
    "        ref_in_pred = int(ref_in_pred)\n",
    "    else:\n",
    "        ref_in_pred = 0\n",
    "    score = {\"ref_in_pred\": ref_in_pred, \"pred_in_ref\": pred_in_ref}\n",
    "    if len(feedback.split(\"[PRED in REF]: \")[-1].split(\"\\n\")[0].split(\";\")):\n",
    "        score[\"detailed_ref_in_pred\"] = \";\".join(\n",
    "            feedback.split(\"[REF in PRED]: \")[-1].split(\"\\n\")[0].split(\";\")[1:]\n",
    "        ).strip()\n",
    "        score[\"detailed_pred_in_ref\"] = \";\".join(\n",
    "            feedback.split(\"[PRED in REF]: \")[-1].split(\"\\n\")[0].split(\";\")[1:]\n",
    "        ).strip()\n",
    "    return score\n",
    "\n",
    "\n",
    "def seperate_text_val(\n",
    "    gt_set, pred_set, path, eval_name, llm_name, mllm_name, save_num, debug_mode=False, eval_type=None\n",
    "):\n",
    "    # scores={user:{} for user in all_users_pred}\n",
    "    demonstrations = open(os.path.join(path, \"templates/ques_evaluation_text.txt\")).readlines()\n",
    "    demonstrations = \"\".join(demonstrations)\n",
    "\n",
    "    if \"gemini\" in eval_name:\n",
    "        from google import genai\n",
    "        from private_info import API_keys\n",
    "        from google.genai.types import HttpOptions\n",
    "\n",
    "        model = genai.Client(http_options=HttpOptions(api_version=\"v1\"), api_key=API_keys.GEMINI_API_KEY)\n",
    "    elif \"gemma\" in eval_name:\n",
    "        import torch\n",
    "        from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "        llm = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "            eval_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(eval_name)\n",
    "        model = {\"model\": llm.eval(), \"processor\": processor}\n",
    "\n",
    "    raw_response = {}\n",
    "    processed_response = {}\n",
    "    for req_id in pred_set:\n",
    "        if \"gemini\" in llm_name:\n",
    "            pred = [row for k, row in enumerate(pred_set[req_id])]\n",
    "        else:\n",
    "            pred = [str(k + 1) + \". \" + row for k, row in enumerate(pred_set[req_id])]\n",
    "        gt = [str(k + 1) + \". \" + row for k, row in enumerate(gt_set[req_id])]\n",
    "\n",
    "        ref = \" \".join(gt)\n",
    "        pred = \" \".join(pred)\n",
    "\n",
    "        print(\"###\", req_id, \"###\")\n",
    "        print(\"GT evid:\\n\\t\", ref)\n",
    "        print(\"Pred evid:\\n\\t\", pred)\n",
    "        incontext_input = gen_incontext_input_textonly(pred, ref, demonstrations)\n",
    "        if \"gemini\" in eval_name:\n",
    "            response = model.models.generate_content(\n",
    "                # model='gemini-2.5-pro-exp-03-25',\n",
    "                model=\"gemini-2.0-flash-001\",\n",
    "                contents=incontext_input,\n",
    "            )\n",
    "            feedback = response.text\n",
    "        elif \"gemma\" in eval_name:\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": incontext_input}]}]\n",
    "            inputs = (\n",
    "                model[\"processor\"]\n",
    "                .apply_chat_template(\n",
    "                    messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n",
    "                )\n",
    "                .to(model[\"model\"].device)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model[\"model\"].generate(**inputs, max_new_tokens=1024)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            feedback = model[\"processor\"].batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "        processed_score = score_extraction(feedback)\n",
    "        raw_response[req_id] = feedback\n",
    "        processed_response[req_id] = processed_score\n",
    "        if debug_mode:\n",
    "            print(feedback, \"\\n\\n\")\n",
    "    pkl.dump(\n",
    "        raw_response,\n",
    "        open(\n",
    "            os.path.join(\n",
    "                path,\n",
    "                \"open_evaluation\",\n",
    "                \"intermediate_info/\"\n",
    "                + \"_\".join([llm_name, mllm_name])\n",
    "                + \"_val_text_\"\n",
    "                + str(save_num)\n",
    "                + \"_raw.pkl\",\n",
    "            ),\n",
    "            \"wb\",\n",
    "        ),\n",
    "    )\n",
    "    pkl.dump(\n",
    "        processed_response,\n",
    "        open(\n",
    "            os.path.join(\n",
    "                path,\n",
    "                \"open_evaluation\",\n",
    "                \"intermediate_info/\"\n",
    "                + \"_\".join([llm_name, mllm_name])\n",
    "                + \"_val_text_\"\n",
    "                + str(save_num)\n",
    "                + \"_processed.pkl\",\n",
    "            ),\n",
    "            \"wb\",\n",
    "        ),\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "def gen_img_text_split(evid_context, pred=False):\n",
    "    inputs = []\n",
    "    for i, evid in enumerate(evid_context):\n",
    "        evid_text = evid[\"text\"] + \" \"\n",
    "        if i == 0 and pred:\n",
    "            evid_text = \"[REF]: \" + evid_text\n",
    "        elif i == 0:\n",
    "            evid_text = \"[PRED]: \" + evid_text\n",
    "        evid_images = evid[\"images\"]\n",
    "        if len(evid_images) == 0:\n",
    "            inputs.append((str(i + 1) + \". \" + evid_text))\n",
    "        else:\n",
    "            img_token_list = re.findall(r\"\\[IMG_.*?\\]\", evid_text)  # [IMG_1], [IMG_2]...\n",
    "            if len(img_token_list) == 0:\n",
    "                inputs.append((str(i + 1) + \". \" + evid_text))\n",
    "            else:\n",
    "                split_string = split_string_by_words(evid_text, img_token_list)\n",
    "                for m, sp_str in enumerate(split_string):\n",
    "                    if sp_str in img_token_list:\n",
    "                        img_idx = re.findall(r\"\\d+\", sp_str)[0]\n",
    "                        inputs.append(Image.open(evid_images[int(img_idx) - 1]).convert(\"RGB\"))\n",
    "                    else:\n",
    "                        if m == 0:\n",
    "                            inputs.append(str(i + 1) + \". \" + sp_str)\n",
    "                        else:\n",
    "                            inputs.append(sp_str)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def val_evid_idv(model, model_name, pred_evid, ref_evid, text_val, seperate_val):\n",
    "    pred = [str(k + 1) + \". \" + row[\"text\"] for k, row in enumerate(pred_evid)]\n",
    "    gt = [str(k + 1) + \". \" + row[\"text\"] for k, row in enumerate(ref_evid)]\n",
    "    ref = \". \".join(gt)\n",
    "    pred = \". \".join(pred)\n",
    "    if text_val or seperate_val:\n",
    "        # print ('GT evid:\\n\\t',ref)\n",
    "        # print ('Pred evid:\\n\\t',pred)\n",
    "        if seperate_val:\n",
    "            # print ('Seperation!')\n",
    "            incontext_input = gen_incontext_input_textonly(pred, ref, seperate_val_demo)\n",
    "        else:\n",
    "            incontext_input = gen_incontext_input_textonly(pred, ref, text_val_demo)\n",
    "        if \"gemini\" in model_name:\n",
    "            response = model.models.generate_content(model=\"gemini-2.0-flash-001\", contents=incontext_input)\n",
    "            feedback = response.text\n",
    "        elif \"gemma\" in model_name:\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": incontext_input}]}]\n",
    "            inputs = (\n",
    "                model[\"processor\"]\n",
    "                .apply_chat_template(\n",
    "                    messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n",
    "                )\n",
    "                .to(model[\"model\"].device)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                generated_ids = model[\"model\"].generate(**inputs, max_new_tokens=1024)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            feedback = model[\"processor\"].batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]\n",
    "    else:\n",
    "        # consider inter-leaved image-text evaluation\n",
    "        inputs = [joint_val_demo]\n",
    "        ref_split = gen_img_text_split(ref_evid)\n",
    "        pred_split = gen_img_text_split(pred_evid, pred=True)\n",
    "        inputs.extend(ref_split)\n",
    "        inputs.extend(pred_split)\n",
    "        if \"gemini\" in model_name:\n",
    "            response = model.models.generate_content(model=\"gemini-2.0-flash-001\", contents=inputs)\n",
    "            feedback = response.text\n",
    "    processed_score = score_extraction(feedback)\n",
    "    return feedback, processed_score\n",
    "\n",
    "\n",
    "def compute_image_scores(model, model_name, pred_evid, ref_evid, score):\n",
    "    prompt = \"Given two sets of images, you need to score how similar they are, ranging from 0-10. The number of images could be different in image sets.\\n\"\n",
    "    prompt += \"[IMG_SET_1]:\"\n",
    "    ref_in_pred = re.findall(r\"\\(.*?\\)\", score[\"detailed_ref_in_pred\"])\n",
    "    pred_in_ref = re.findall(r\"\\(.*?\\)\", score[\"detailed_pred_in_ref\"])\n",
    "    print(\"ref in pred:\", ref_in_pred, \"\\n pred in ref\", pred_in_ref)\n",
    "    image_scores = {\"pred_in_ref\": [], \"ref_in_pred\": []}\n",
    "    # print (ref_in_pred)\n",
    "    # print (pred_in_ref)\n",
    "    for detail in pred_in_ref:\n",
    "        info = detail[1:-1].split(\",\")\n",
    "        # print ('pred in ref:',info)\n",
    "        try:\n",
    "            pred_idx = int(info[0].split(\"_\")[-1])\n",
    "            ref_idx = int(info[1].split(\"_\")[-1])\n",
    "            imgs_pred = pred_evid[pred_idx - 1][\"images\"]\n",
    "            imgs_ref = ref_evid[ref_idx - 1][\"images\"]\n",
    "            if len(imgs_pred) == 0 or len(imgs_ref) == 0:\n",
    "                feedback = \"10\"\n",
    "            else:\n",
    "                if \"gemini\" in model_name:\n",
    "                    inputs = [prompt]\n",
    "                    for img in imgs_pred:\n",
    "                        inputs.append(Image.open(img).convert(\"RGB\"))\n",
    "                    inputs.append(\"\\n[IMG_SET_2]:\")\n",
    "                    for img in imgs_ref:\n",
    "                        inputs.append(Image.open(img).convert(\"RGB\"))\n",
    "                    inputs.append(\"\\nPlease generate your rating with one integer:\")\n",
    "                    response = model.models.generate_content(model=\"gemini-2.0-flash-001\", contents=inputs)\n",
    "                    feedback = response.text\n",
    "                elif \"gemma\" in model_name:\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                        }\n",
    "                    ]\n",
    "                    for img in imgs_pred:\n",
    "                        messages[0][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
    "                    messages[0][\"content\"].append({\"type\": \"text\", \"text\": \"\\n[IMG_SET_2]:\"})\n",
    "                    for img in imgs_ref:\n",
    "                        messages[0][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
    "                    messages[0][\"content\"].append(\n",
    "                        {\"type\": \"text\", \"text\": \"\\nPlease generate your rating with one integer:\"}\n",
    "                    )\n",
    "                    inputs = (\n",
    "                        model[\"processor\"]\n",
    "                        .apply_chat_template(\n",
    "                            messages,\n",
    "                            add_generation_prompt=True,\n",
    "                            tokenize=True,\n",
    "                            return_dict=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                        .to(model[\"model\"].device)\n",
    "                    )\n",
    "                    with torch.no_grad():\n",
    "                        generated_ids = model[\"model\"].generate(**inputs, max_new_tokens=1024)\n",
    "                    generated_ids_trimmed = [\n",
    "                        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                    ]\n",
    "                    feedback = model[\"processor\"].batch_decode(\n",
    "                        generated_ids_trimmed, skip_special_tokens=True\n",
    "                    )[0]\n",
    "        except:\n",
    "            print(\"##Edge case image!!\")\n",
    "            feedback = \"10\"\n",
    "        image_scores[\"pred_in_ref\"].append({\"info\": info, \"score\": feedback})\n",
    "    for detail in ref_in_pred:\n",
    "        info = detail[1:-1].split(\",\")\n",
    "        # print ('ref in pred',info)\n",
    "        try:\n",
    "            pred_idx = int(info[1].split(\"_\")[-1])\n",
    "            ref_idx = int(info[0].split(\"_\")[-1])\n",
    "            imgs_pred = pred_evid[pred_idx - 1][\"images\"]\n",
    "            imgs_ref = ref_evid[ref_idx - 1][\"images\"]\n",
    "            if len(imgs_pred) == 0 or len(imgs_ref) == 0:\n",
    "                feedback = \"10\"\n",
    "            else:\n",
    "                if \"gemini\" in model_name:\n",
    "                    inputs = [prompt]\n",
    "                    for img in imgs_pred:\n",
    "                        inputs.append(Image.open(img).convert(\"RGB\"))\n",
    "                    inputs.append(\"\\n[IMG_SET_2]:\")\n",
    "                    for img in imgs_ref:\n",
    "                        inputs.append(Image.open(img).convert(\"RGB\"))\n",
    "                    inputs.append(\"\\nPlease generate your rating with one integer:\")\n",
    "                    response = model.models.generate_content(model=\"gemini-2.0-flash-001\", contents=inputs)\n",
    "                    feedback = response.text\n",
    "                elif \"gemma\" in model_name:\n",
    "                    messages = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "                        }\n",
    "                    ]\n",
    "                    for img in imgs_pred:\n",
    "                        messages[0][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
    "                    messages[0][\"content\"].append({\"type\": \"text\", \"text\": \"\\n[IMG_SET_2]:\"})\n",
    "                    for img in imgs_ref:\n",
    "                        messages[0][\"content\"].append({\"type\": \"image\", \"image\": img})\n",
    "                    messages[0][\"content\"].append(\n",
    "                        {\"type\": \"text\", \"text\": \"\\nPlease generate your rating with one integer:\"}\n",
    "                    )\n",
    "                    inputs = (\n",
    "                        model[\"processor\"]\n",
    "                        .apply_chat_template(\n",
    "                            messages,\n",
    "                            add_generation_prompt=True,\n",
    "                            tokenize=True,\n",
    "                            return_dict=True,\n",
    "                            return_tensors=\"pt\",\n",
    "                        )\n",
    "                        .to(model[\"model\"].device)\n",
    "                    )\n",
    "                    with torch.no_grad():\n",
    "                        generated_ids = model[\"model\"].generate(**inputs, max_new_tokens=1024)\n",
    "                    generated_ids_trimmed = [\n",
    "                        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "                    ]\n",
    "                    feedback = model[\"processor\"].batch_decode(\n",
    "                        generated_ids_trimmed, skip_special_tokens=True\n",
    "                    )[0]\n",
    "        except:  # https://github.com/abril4416/AVerImaTeC?tab=readme-ov-file\n",
    "            print(\"##Edge case image!!\")\n",
    "            feedback = \"10\"\n",
    "        image_scores[\"ref_in_pred\"].append({\"info\": info, \"score\": feedback})\n",
    "    return image_scores\n",
    "\n",
    "\n",
    "def textual_val_single(ref, pred, path, eval_name, model, eval_type=\"\", debug_mode=False):\n",
    "    if eval_type == \"justification\":\n",
    "        val_demo = justi_val_demo\n",
    "    elif eval_type == \"question\":\n",
    "        val_demo = ques_val_demo\n",
    "        if pred[0][0].isdigit() == False:\n",
    "            pred = [str(k + 1) + \". \" + row for k, row in enumerate(pred)]\n",
    "        pred = \" \".join(pred)\n",
    "        ref = [str(k + 1) + \". \" + row for k, row in enumerate(ref)]\n",
    "        ref = \" \".join(ref)\n",
    "\n",
    "    incontext_input = gen_incontext_input_textonly(pred, ref, val_demo)\n",
    "    if \"gemini\" in eval_name:\n",
    "        response = model.models.generate_content(\n",
    "            # model='gemini-2.5-pro-exp-03-25',\n",
    "            model=\"gemini-2.0-flash-001\",\n",
    "            contents=incontext_input,\n",
    "        )\n",
    "        feedback = response.text\n",
    "    elif \"gemma\" in eval_name:\n",
    "        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": incontext_input}]}]\n",
    "        inputs = (\n",
    "            model[\"processor\"]\n",
    "            .apply_chat_template(\n",
    "                messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            .to(model[\"model\"].device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model[\"model\"].generate(**inputs, max_new_tokens=1024)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        feedback = model[\"processor\"].batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]\n",
    "\n",
    "    processed_score = score_extraction(feedback)\n",
    "\n",
    "    if debug_mode:\n",
    "        print(eval_type)\n",
    "        print(processed_score)\n",
    "        print(feedback, \"\\n\\n\")\n",
    "\n",
    "    return feedback, processed_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
